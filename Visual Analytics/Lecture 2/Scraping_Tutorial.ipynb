{"cells":[{"cell_type":"markdown","id":"9618310d","metadata":{"id":"9618310d"},"source":["# Web Scraping Tutorial\n","\n","This tutorial will teach you how Python to scrap and extract data from a web page. We will use two packages, `requests` to scrap the webpage and `BeautifulSoup` to extract the data.\n","\n","Many good references on web scraping are available online. I would recommend the following resources:\n","1. Automate Boring Stuff with Python by Al Sweigart (2020) has a chapter on Web Scraping tutorial, which can be read [online](https://automatetheboringstuff.com/2e/chapter12/).\n","2. Web Scraping With Python by Ryan Mitchell (2018) is a bit old book but provides a comprehensive guide to the topic.\n","\n","**Goal:** We will extract the cryptocurrency market price from Etherscan website: https://etherscan.io/tokens\n","\n","Your first step should always be to familiarize yourself with the website you want to scrape. Take a look at the website and try to inspect the HTML elements on the webpage."]},{"cell_type":"markdown","id":"d17dfa0c","metadata":{"id":"d17dfa0c"},"source":["## Step 1: Scrap a web page\n","\n","Now, we are ready to scrap a webpage we want to get the data from with the `requests` package. We will use the following functions:\n","\n","* `requests.get('URL')` - make a request to the specified URL\n","* `r.status_code` - get the status code of the request\n","* `r.content` - get the binary content of the page\n","\n","More functions in the `requests` package are available in [its documentation](https://requests.readthedocs.io/en/latest/)."]},{"cell_type":"code","execution_count":5,"id":"0d4756eb","metadata":{"id":"0d4756eb"},"outputs":[],"source":["# First, we will import the requests package\n","import requests\n","import time"]},{"cell_type":"code","execution_count":9,"id":"8c15921a","metadata":{"id":"8c15921a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully fetched the webpage\n"]}],"source":["# Request the webpage\n","headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n","url = 'https://www.linkedin.com/jobs/search/?keywords=software%20engineer'\n","response = requests.get(url, headers=headers)\n","if response.status_code == 200:\n","    print(\"Successfully fetched the webpage\")\n","elif response.status_code == 429:\n","    print(\"Rate limit exceeded. Retrying after a delay...\")\n","    time.sleep(10)  # Wait for 10 seconds before retrying\n","    response = requests.get(url, headers=headers)\n","    if response.status_code == 200:\n","        print(\"Successfully fetched the webpage after retrying\")\n","    else:\n","        print(f\"Failed to fetch the webpage after retrying. Status code: {response.status_code}\")\n","    print(\"Successfully fetched the webpage\")\n","else:\n","    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n","\n","    # Check the content of the webpage\n","    print(response.content[:500])  # Print the first 500 characters of the content"]},{"cell_type":"code","execution_count":10,"id":"04ba2a22","metadata":{"id":"04ba2a22"},"outputs":[{"name":"stdout","output_type":"stream","text":["b'<!DOCTYPE html>\\n\\n    \\n    \\n    \\n    \\n    \\n    \\n\\n    \\n    \\n    \\n    \\n\\n    \\n    \\n    \\n    \\n\\n    \\n    <html lang=\"en\">\\n      <head>\\n        <meta name=\"pageKey\" content=\"d_jobs_guest_search\">\\n<!---->          <meta name=\"linkedin:pageTag\" content=\"urlType=jserp_custom;emptyResult=false\">\\n        <meta name=\"locale\" content=\"en_US\">\\n        <meta id=\"config\" data-app-version=\"2.0.2056\" data-call-tree-id=\"AAYjj9N2DtnexTcI4B9PjQ==\" data-multiproduct-name=\"jobs-guest-frontend\" data-service-name=\"jobs-g'\n"]}],"source":["print(response.content[:500])  "]},{"cell_type":"code","execution_count":20,"id":"3cb0b645","metadata":{"id":"3cb0b645"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error occurred: HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /jobs?q=Python+Developer&l=London&start=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x77150c047310>, 'Connection to 123.45.67.89 timed out. (connect timeout=5)'))\n","Retrying... (1/3)\n","Error occurred: HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /jobs?q=Python+Developer&l=London&start=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x771501161960>, 'Connection to 123.45.67.89 timed out. (connect timeout=5)'))\n","Retrying... (2/3)\n","Error occurred: HTTPSConnectionPool(host='www.indeed.com', port=443): Max retries exceeded with url: /jobs?q=Python+Developer&l=London&start=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x771501161540>, 'Connection to 123.45.67.89 timed out. (connect timeout=5)'))\n","Max retries exceeded. Skipping this page.\n"]},{"ename":"UnboundLocalError","evalue":"local variable 'response' referenced before assignment","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 使用示例，爬取 \"Python Developer\" 在 \"London\" 的职位信息\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mscrape_indeed_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPython Developer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLondon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 42\u001b[0m, in \u001b[0;36mscrape_indeed_jobs\u001b[0;34m(keyword, location, num_pages)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 检查状态码\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     43\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     job_cards \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_seen_beacon\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 根据 CSS 类选择职位卡片\u001b[39;00m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'response' referenced before assignment"]}],"source":["import random\n","import time\n","from bs4 import BeautifulSoup\n","\n","# 随机选择一个代理\n","def get_random_proxy():\n","    # Exclude the problematic proxy\n","    valid_proxies = [proxy for proxy in proxies if proxy['http'] != 'http://98.76.54.32:8080']\n","    return random.choice(valid_proxies)\n","\n","# 构建 Indeed 搜索 URL\n","def build_indeed_url(keyword, location, page):\n","    base_url = \"https://www.indeed.com/jobs\"\n","    params = {\n","        \"q\": keyword,     # 搜索关键词\n","        \"l\": location,    # 搜索位置\n","        \"start\": page * 10  # Indeed 每页有 10 个结果\n","    }\n","    return base_url, params\n","\n","# 爬取 Indeed 工作信息\n","def scrape_indeed_jobs(keyword, location, num_pages=1):\n","    for page in range(num_pages):\n","        url, params = build_indeed_url(keyword, location, page)\n","        proxy = get_random_proxy()  # 使用随机代理\n","        retries = 3  # 设置重试次数\n","        for attempt in range(retries):\n","            try:\n","                response = requests.get(url, headers=headers, params=params, proxies=proxy, timeout=(5, 10))  # 设置超时时间\n","                if response.status_code == 200:\n","                    break  # 成功获取响应，跳出重试循环\n","            except requests.exceptions.RequestException as e:\n","                print(f\"Error occurred: {e}\")\n","                if attempt < retries - 1:\n","                    print(f\"Retrying... ({attempt + 1}/{retries})\")\n","                    time.sleep(2)  # 等待 2 秒后重试\n","                else:\n","                    print(\"Max retries exceeded. Skipping this page.\")\n","                    continue\n","\n","        # 检查状态码\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","            job_cards = soup.find_all('div', class_='job_seen_beacon')  # 根据 CSS 类选择职位卡片\n","\n","            for job_card in job_cards:\n","                # 提取职位标题\n","                job_title_tag = job_card.find('h2', class_='jobTitle')\n","                job_title = job_title_tag.text if job_title_tag else 'N/A'\n","\n","                # 提取公司名称\n","                company_tag = job_card.find('span', class_='companyName')\n","                company_name = company_tag.text if company_tag else 'N/A'\n","\n","                # 提取工作地点\n","                location_tag = job_card.find('div', class_='companyLocation')\n","                location = location_tag.text if location_tag else 'N/A'\n","\n","                # 提取发布时间\n","                post_time_tag = job_card.find('span', class_='date')\n","                post_time = post_time_tag.text if post_time_tag else 'N/A'\n","\n","                print(f\"职位: {job_title}\")\n","                print(f\"公司: {company_name}\")\n","                print(f\"地点: {location}\")\n","                print(f\"发布时间: {post_time}\")\n","                print(\"-\" * 50)\n","        else:\n","            print(f\"Failed to retrieve page {page + 1}, status code: {response.status_code}\")\n","\n","        # 在每次请求之间添加随机延迟，模拟真人操作\n","        delay = random.uniform(2, 6)  # 随机延迟 2 到 6 秒\n","        print(f\"等待 {delay:.2f} 秒...\")\n","        time.sleep(delay)\n","\n","# 使用示例，爬取 \"Python Developer\" 在 \"London\" 的职位信息\n","scrape_indeed_jobs(\"Python Developer\", \"London\", num_pages=2)\n"]},{"cell_type":"code","execution_count":22,"id":"74151d99","metadata":{"id":"74151d99"},"outputs":[{"ename":"HTTPError","evalue":"HTTP Error 403: Forbidden","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m source \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://fr.indeed.com/?advn=5964528717357619&vjk=376d6a346093a302\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url \u001b[38;5;241m=\u001b[39m source, headers \u001b[38;5;241m=\u001b[39m headers)\n\u001b[0;32m---> 15\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Create a parse tree with beautifulsoup\u001b[39;00m\n\u001b[1;32m     18\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs\u001b[38;5;241m.\u001b[39mBeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n","\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"]}],"source":["# Get the header of the web page\n","#Beautiful Soup project\n","\n","import bs4 as bs\n","from urllib.request import urlopen, Request\n","import pandas as pd\n","import re\n","\n","#Create a header to prevent 404 error. This is necessary on sites like Angelist.\n","headers={'User-Agent': 'Mozilla/5.0'}\n","\n","#Request an Indeed.com Webpage\n","source = ('https://fr.indeed.com/?advn=5964528717357619&vjk=376d6a346093a302')\n","req = Request(url = source, headers = headers)\n","html = urlopen(req).read()\n","\n","#Create a parse tree with beautifulsoup\n","soup = bs.BeautifulSoup(html, 'lxml')\n","\n","\n","#Get all Job Titles\n","tagarray = []\n","\n","#Search through bs parse tree to find text with the below properties (which are all the job titles)\n","for tag in soup.findAll('a', {'target': \"_blank\", 'title': True, 'data-tn-element':\"jobTitle\"}):\n","    tagarray.append(tag.get_text()) #output text-based results to array\n","\n","jobtitles = pd.DataFrame(data = tagarray) #output to pandas df\n","\n","\n","#Get all Job Links\n","urlarray = []\n","\n","#Search through bs parse tree to find links in href tag with 'clk' in them\n","for url in soup.findAll('a', {'href': re.compile('clk')}, {'href': re.compile('company')}):\n","    urlarray.append(\"www.indeed.com\" + url.get('href')) #add 'www.indeed.com' to the href and append to array\n","\n","joblinks = pd.DataFrame(data = urlarray) #ouput to pandas df\n","\n","    \n","#Join DataFrames and rename columns\n","result = pd.concat([jobtitles, joblinks], axis = 1)\n","result.columns = ['Job Titles', 'Job Links'] \n","\n","#Print and output to a CSV\n","print(result)\n","result.to_csv('beautifulsoup.csv')"]},{"cell_type":"code","execution_count":null,"id":"63b8bf8d","metadata":{"id":"63b8bf8d"},"outputs":[],"source":["# Get the content of the web page\n"]},{"cell_type":"code","execution_count":null,"id":"d9341389","metadata":{"id":"d9341389"},"outputs":[],"source":["# Get the text in the web page\n"]},{"cell_type":"code","execution_count":null,"id":"5938e1df","metadata":{"id":"5938e1df"},"outputs":[],"source":["# Save the content of web page\n"]},{"cell_type":"markdown","id":"130d01f8","metadata":{"id":"130d01f8"},"source":["## Step 2: Load the web page as BeautifulSoup object\n","\n","After we crawled the web page and download it to the local disk, we will use `BeautifulSoup` package to parse HTML file and access the content. We will use the following functions:\n","\n","**1. Load the web page to BeautifulSoup**\n","* `soup = BeautifulSoup(html_doc, 'html.parser')` - parse the HTML content to BeautifulSoup object"]},{"cell_type":"code","execution_count":null,"id":"8b2eef79","metadata":{"id":"8b2eef79"},"outputs":[],"source":["# First, we will import the BeautifulSoup from bs4 package\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":null,"id":"29460bba","metadata":{"id":"29460bba"},"outputs":[],"source":["# Load the web page and parse it to BeautifulSoup\n"]},{"cell_type":"code","execution_count":null,"id":"c9f7ab9a","metadata":{"id":"c9f7ab9a"},"outputs":[],"source":["# Check the type of our soup object\n"]},{"cell_type":"code","execution_count":null,"id":"5a89243d","metadata":{"id":"5a89243d"},"outputs":[],"source":["# Print the content of the web page\n"]},{"cell_type":"code","execution_count":null,"id":"9ee1f8e4","metadata":{"id":"9ee1f8e4"},"outputs":[],"source":["# Print all the text in the webpage\n"]},{"cell_type":"markdown","id":"LjNxOJUEyuPY","metadata":{"id":"LjNxOJUEyuPY"},"source":["**2. Get the content of the element**\n","* `soup.title` - get the title of the page\n","* `soup.title.string` - get the string in the title element\n","* `soup.h1` - get the H1 element in the web page\n","* `soup.h1.attrs` - get all attributes in the H1 element\n","* `soup.h1['class']` - get the class attribute in the H1 element"]},{"cell_type":"code","execution_count":null,"id":"ae45ff28","metadata":{"id":"ae45ff28"},"outputs":[],"source":["# Get the title of the page\n"]},{"cell_type":"code","execution_count":null,"id":"ecb4ef3b","metadata":{"id":"ecb4ef3b"},"outputs":[],"source":["# Other HTML elements also work too\n"]},{"cell_type":"code","execution_count":null,"id":"4107df52","metadata":{"id":"4107df52"},"outputs":[],"source":["# Get the class attribute of an element\n"]},{"cell_type":"markdown","id":"wIsB9X-Jyz5b","metadata":{"id":"wIsB9X-Jyz5b"},"source":["**3. Look for the element in the web page**\n","* `soup.find('HTML_tag')` - get the element from an HTML tag\n","* `soup.find_all('HTML_tag')` - get the list of elelemts that has the specified HTML tag\n","* `soup.select('CSS_selector')` - get the list of elements with the specified [CSS selector](https://www.w3schools.com/cssref/css_selectors.asp)"]},{"cell_type":"code","execution_count":null,"id":"d370ecc0","metadata":{"id":"d370ecc0"},"outputs":[],"source":["# We can also get the page title using soup.find() function\n"]},{"cell_type":"code","execution_count":null,"id":"9reBVSAuzRkK","metadata":{"id":"9reBVSAuzRkK"},"outputs":[],"source":["# Get all the elements with image tag\n"]},{"cell_type":"code","execution_count":null,"id":"fiu6CYDGocEi","metadata":{"id":"fiu6CYDGocEi"},"outputs":[],"source":["# Get all the token names on the web page\n"]},{"cell_type":"markdown","id":"u4qcpEWJy9mp","metadata":{"id":"u4qcpEWJy9mp"},"source":["## Step 3: Extract the data from the table\n","\n","Now, we will extract the cryptocurrencies market price from the table."]},{"cell_type":"code","execution_count":null,"id":"8e54b736","metadata":{"id":"8e54b736"},"outputs":[],"source":["# Get the table element in the web page\n"]},{"cell_type":"code","execution_count":null,"id":"69ee70a8","metadata":{"id":"69ee70a8"},"outputs":[],"source":["# Get the table headers\n"]},{"cell_type":"markdown","id":"6qb-NIfV1kxL","metadata":{"id":"6qb-NIfV1kxL"},"source":["For loop over each row in the table and extract the data for each column in the row."]},{"cell_type":"code","execution_count":null,"id":"rHg-Ri_S3_SN","metadata":{"id":"rHg-Ri_S3_SN"},"outputs":[],"source":["# For loop over each row in the table\n","\n","\n","    # Get all the columns in the row\n","\n","\n","    # For loop over each column and extract the string\n","\n"]},{"cell_type":"markdown","id":"CYIamv6e-lOH","metadata":{"id":"CYIamv6e-lOH"},"source":["## Step 4: Create a DataFrame table and write to a CSV file"]},{"cell_type":"code","execution_count":null,"id":"BvFNIO2x_M2P","metadata":{"id":"BvFNIO2x_M2P"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"YoPXwFOb9rvE","metadata":{"id":"YoPXwFOb9rvE"},"outputs":[],"source":["# How many rows in the extracted data\n"]},{"cell_type":"code","execution_count":null,"id":"y4xoW-fQ9xnT","metadata":{"id":"y4xoW-fQ9xnT"},"outputs":[],"source":["# Convert the data list to DataFrame object\n"]},{"cell_type":"markdown","id":"vuGWTQ4LCg0l","metadata":{"id":"vuGWTQ4LCg0l"},"source":["Split the columns with \"\\n\""]},{"cell_type":"code","execution_count":null,"id":"CxSweamlCf8O","metadata":{"id":"CxSweamlCf8O"},"outputs":[],"source":["# Split between token name and token symbol\n"]},{"cell_type":"code","execution_count":null,"id":"s5n2YlKA_yUd","metadata":{"id":"s5n2YlKA_yUd"},"outputs":[],"source":["# Split between the USD and ETH prices\n"]},{"cell_type":"code","execution_count":null,"id":"pdIUxN4yACgS","metadata":{"id":"pdIUxN4yACgS"},"outputs":[],"source":["# Split the number of holders and percent changes\n"]},{"cell_type":"markdown","id":"YplcV-TmEvF7","metadata":{"id":"YplcV-TmEvF7"},"source":["Convert string into numerical columns"]},{"cell_type":"code","execution_count":null,"id":"BkIsQ83uPjcR","metadata":{"id":"BkIsQ83uPjcR"},"outputs":[],"source":["# Regular expression pattern to match numbers\n","pattern = r'([-+]?\\d[\\d,]*(?:\\.\\d+)?)'"]},{"cell_type":"code","execution_count":null,"id":"37WDkw6LGWtu","metadata":{"id":"37WDkw6LGWtu"},"outputs":[],"source":["# For each numerical column, convert the string to float numbers\n","\n","\n","    # Use df[col_name].str.extract() to extract the numbers and\n","    # .astype(float) to convert the string to float numbers\n","\n","\n"]},{"cell_type":"markdown","id":"u4MSGKsjHl3o","metadata":{"id":"u4MSGKsjHl3o"},"source":["Last but not least, remove the bracket in token symbol column"]},{"cell_type":"code","execution_count":null,"id":"01EdKFSLGk80","metadata":{"id":"01EdKFSLGk80"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"LcGeD1xOQFIb","metadata":{"id":"LcGeD1xOQFIb"},"source":["Write the DataFrame table to CSV"]},{"cell_type":"code","execution_count":null,"id":"DavpYCJHIvCG","metadata":{"id":"DavpYCJHIvCG"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
